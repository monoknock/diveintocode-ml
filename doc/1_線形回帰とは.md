# 線形回帰とは

## ゴール
- **【Sprint 機械学習スクラッチ 線形回帰】問題１を解くうえで必要な知識や技術について理解する**

## このヒント集について
### Sprintの目的
- 線形回帰の意味を理解する
- 線形回帰をスクラッチ開発するのに必要な概念を理解する

## どのように学ぶか

【Sprint 機械学習スクラッチ 線形回帰】の目次と照らし合わせながら、進めていきましょう。

### 線形回帰とは
**線形回帰**とは、目的変数(y)と説明変数(x)を$y=ax+b$という関係式で近似的に当てはめることです。

$y=ax+b$と書ける式はxy座標で直線を描きます。以下画像のようなイメージです。

<a href="https://diveintocode.gyazo.com/f07c4804230471314cc50ba61b0dd627"><img src="https://t.gyazo.com/teams/diveintocode/f07c4804230471314cc50ba61b0dd627.png" alt="Image from Gyazo" width="800"/></a>


### スクラッチコードの完成形
線形回帰のクラスをスクラッチで作成していきますが、最終的なコードはどのようになっているのでしょうか。

下記は、最終的なコードの概観になります。

```python
class ScratchLinearRegression():
    def __init__(self,・・・):
      """
      インスタンス変数初期化
      """
      ・・・

    # 問題6（学習と推定）
    def fit(self,・・・):
        """
        線形回帰の学習
        """
        # メイン処理
        for i in range(学習回数):
            # 問題1（過程関数の計算）
            self._linear_hypothesis(・・・)
            
            # 問題2（最急降下法によるパラメータの更新値計算）
            self._gradient_descent(・・・)

            # 問題7（学習曲線のプロット）のグラフ描画時（問題5（損失関数）で作成した関数を使用）
            self._loss_func(・・・)


    # 問題1
    def _linear_hypothesis(self,・・・):
        """
        仮定関数の計算
        """

    # 問題2
    def _gradient_descent(self,・・・):
        """
        最急降下法によるパラメータの更新値計算
        """

    # 問題3
    def predict(self,・・・):
        """
        線形回帰での推定
        """

    # 問題4
    def _mse(self,・・・):
        """
        平均二乗誤差の計算
        """

    # 問題5
    def _loss_func(self,・・・):
        """
        損失関数
        """
        # 問題4
        self._mse(・・・)
        

```

### 線形回帰のイメージ

HousePriceデータセットで考えてみます。

目的変数`y(HousePrice)`に対し、ひとつ説明変数`x1(GrLivArea)`を選び、2変数間の関係をプロットしてみると下記のようなグラフになります。

<a href="https://diveintocode.gyazo.com/b3e0593438c1986b900f7da3bc02a7e0"><img src="https://t.gyazo.com/teams/diveintocode/b3e0593438c1986b900f7da3bc02a7e0.png" alt="Image from Gyazo" width="800"/></a>


プロットされたデータの傾向をみると、「なにやら直線が引けそう」ということがわかるかと思います。これを**線形関係にある**と言い、下記のような直線を引くことが出来ます。

<a href="https://diveintocode.gyazo.com/dee97b529a9956d82d375c10f88056a8"><img src="https://t.gyazo.com/teams/diveintocode/dee97b529a9956d82d375c10f88056a8.png" alt="Image from Gyazo" width="800"/></a>


上記のグラフの直線を一般化して数式に直すと、次のようになります。

$$
y = a x_1 + b
$$

データに対して、最も当てはまりの良い`a`と`b`を求めることを、**回帰式を解く**と言います。そして、この式は、説明変数の数が1つの場合の式なので、**線形単回帰**になります。

## まとめ
- 線形回帰とは目的変数(y)が説明変数(x)にどれほど依存しているかあらわすモデルです
- 説明変数が1つのものを線形単回帰モデルと呼び、2つ以上あるものを線形重回帰モデルと呼びます
- 線形単回帰モデルは一般に$y = a x_1 + b$と記述できます