# 最急降下法とは

## ゴール
- **【Sprint 機械学習スクラッチ 線形回帰】の【問題2】を解くうえで必要な知識や技術について理解する**

### Sprintの目的
- 線形回帰の意味を理解する
- 線形回帰をスクラッチ開発するのに必要な概念を理解する

## どのように学ぶか

【Sprint 機械学習スクラッチ 線形回帰】の目次と照らし合わせながら、進めていきましょう。

## 【問題2】最急降下法
**回帰式を解く**とは、データにもっとも当てはまりの良い、Θi(i=複数変数ある際の変数の番号)の値を求めることだと紹介しましたが、どのように、このΘを求めればいいのでしょうか。

### 誤差

**データにもっとも当てはまりが良い**とは、下記の図で言う`error`の値が、最も小さくなる時のことを言います。

<a href="https://diveintocode.gyazo.com/dc4108d0eea9b579cb67ebbd8a5a7d92"><img src="https://t.gyazo.com/teams/diveintocode/dc4108d0eea9b579cb67ebbd8a5a7d92.png" alt="Image from Gyazo" width="800"/></a>

### 平均2乗誤差

引く直線によっては、`error`の値が±両方出てきますので、単純なerrorの合計値だけで、**データにもっとも当てはまりが良い**状態を判断することが出来ません。

そこで、使用されるのが、平均2乗誤差と呼ばれるもので、下記の式で表されます。

$$
L(\theta)=  \frac{1 }{ m}  \sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})^2.
$$

この数式の値が最小になるようなΘを求めることが、回帰式の**ゴール**となります。

### 最急降下法（Θの求め方）

例えば、平均2乗誤差の数式をグラフにプロットした場合、下記のようになったとします。

<a href="https://diveintocode.gyazo.com/607d80ca9eebee99b564c1e47c946896"><img src="https://t.gyazo.com/teams/diveintocode/607d80ca9eebee99b564c1e47c946896.png" alt="Image from Gyazo" width="800"/></a>

平均2乗誤差の値が最小になる地点は、次の位置であるとわかるかと思います。

<a href="https://diveintocode.gyazo.com/704481108d13936ee159071457fd114c"><img src="https://t.gyazo.com/teams/diveintocode/704481108d13936ee159071457fd114c.png" alt="Image from Gyazo" width="800"/></a>

このΘの個所をどのように特定すればいいのでしょうか。下記の手順になります。

①まずは、ランダムにΘをプロットします。

<a href="https://diveintocode.gyazo.com/2dcbcbe726bcb342cc003fccbfebf8a3"><img src="https://t.gyazo.com/teams/diveintocode/2dcbcbe726bcb342cc003fccbfebf8a3.png" alt="Image from Gyazo" width="800"/></a>

②ランダムにプロットされたシータの位置の傾きを求める

<a href="https://diveintocode.gyazo.com/b691334882a9eb7de2ea145e62a8e28d"><img src="https://t.gyazo.com/teams/diveintocode/6b691334882a9eb7de2ea145e62a8e28d.png" alt="Image from Gyazo" width="800"/></a>

③傾きと学習率により、Θを更新

<a href="https://diveintocode.gyazo.com/60b2742d1713b8c3fef99b38ec62443b"><img src="https://t.gyazo.com/teams/diveintocode/60b2742d1713b8c3fef99b38ec62443b.png" alt="Image from Gyazo" width="800"/></a>

④①～③を繰り返す

<a href="https://diveintocode.gyazo.com/328d2159009b54896252805af22e0f84"><img src="https://t.gyazo.com/teams/diveintocode/328d2159009b54896252805af22e0f84.png" alt="Image from Gyazo" width="800"/></a>

**これを数式で表すと下記のようになります。**

$$
J(\theta)=  \frac{1 }{ 2m}  \sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})^2.
$$

$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m}[(h_\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]
$$


- ヒント：パラメータ導出のイメージ（https://diver.diveintocode.jp/questions/8028）
- ヒント：なぜ降下法を使うのか（https://diver.diveintocode.jp/questions/8029）


### 損失関数（目的関数）

平均2乗誤差を最小にするΘを求めてきましたが、この対象となる誤差関数のことを、**損失関数（目的関数）**といいます。

Θの更新の際に、平均2乗誤差を使用していると紹介しましたが、厳密には、展開後の式を分かりやすくするため、下記の数式を利用しています。

$$
L(\theta)=  \frac{1 }{ m}  \sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})^2.
$$

↓

$$
J(\theta)=  \frac{1 }{ 2m}  \sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})^2.
$$

どちらもΘについて微分し、係数を学習率に飲み込ませることで等価な式となります。

### トイデータ

作成した関数に、下記の変数を引数として与えてみましょう。

まずは誤差の含まないモデルで実験してみましょう。
例えば適当に以下のデータを利用します。
```python
x = np.linspace(1,6,5)
X = np.c_[np.ones(5),x]#入力データX

y = 2*x + 1#適当な真のモデル

theta = [0,0]#仮定関数の係数の初期値
y_pred = X @ theta

error = y_pred - y#入力データerror
```

更新式を実装し、学習率alpha=0.05の時、更新された後のthetaが以下の様になっていれば成功です。

```
theta:[0.8, 3.1450000000000005]
```
以下の更新式を、行列計算で書いた場合は
$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m}[(h_\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]
$$

```
theta:[0.8, 3.425]
```
となります。

この違いはパラメータ更新を同時に行うか、1つずつ行うかという違いになります。

## まとめ
- 最急降下法とは`y = ax1 + b`における`a`と`b`を最適な値にする手法です
- 平均2乗誤差(損失関数)を微分することで更新式が導出されることを学びました
- 更新過程のイメージは平均2乗誤差の山を降っていく様なイメージです