# 仮定関数とは

## ゴール
- **【Sprint 機械学習スクラッチ 線形回帰】の【問題１】を解くうえで必要な知識や技術について理解する**

### Sprintの目的
- 仮定関数の意味を理解する
- 線形回帰をスクラッチ開発するのに必要な転置行列や行列積について理解する
- 線形回帰をスクラッチ実装する際のより踏み込んだヒントを掲載しているので、それを基に、仮定関数をスクラッチ実装できるようになる。

## どのように学ぶか

【Sprint 機械学習スクラッチ 線形回帰】の【問題１】と照らし合わせながら、進めていきましょう。

## 仮定関数とは

これまで学んできた`y = ax1 + b`を`仮定関数`と呼びます。

上記の例では、変数が1つだけでしたが、複数の変数がある**線形重回帰**の場合、下記のような式になります。

$$
h_\theta(x) =  \theta_0 x_0 + \theta_1 x_1 + ... + \theta_j x_j + ... +\theta_n x_n.   (x_0 = 1)\\
$$å

これを変形すると下記のようになります。

$$
h_\theta(x) = \theta^T \cdot x.
$$

この数式は、どのように導出されたのでしょうか。

### 転置行列積

転置行列積が`y = ax1 + b`の計算結果と一致することを確認してみましょう。
```python
import numpy as np

#y=ax1+b
a = 1
b = 2
x1 = 3
y = a*x1 + b
print(y)

#転置行列積
theta = np.array([[2],[1]])
X = np.array([[1,3]])
y = theta.T @ X
print(y)
```
bにあたる部分をthetaの第1成分とし,入力Xの第1成分を1とすることで等価な処理を行えます。
行列計算に置き換えることで、計算時間が短縮されるので、行列計算に落とし込める場合はできるだけ、行列計算に落とし込みましょう。

また、機械学習では2つのデータだけでなく複数のデータを一挙に取り扱う事が多いです。そういった場合にはXは2次元配列になります。
以下のコードで実験してみましょう。

```python
X = np.array([[1,1],
              [1,2],
              [1,3]])
y = theta.T @ X.T
```


つまり、
$$
h_\theta(x) =  \theta_0 x_0 + \theta_1 x_1 + ... + \theta_j x_j + ... +\theta_n x_n.   (x_0 = 1)\\
$$
は
$$
h_\theta(x) = \theta^T \cdot x.
$$
と記述できます（記述方法に関しては、数学の世界で決まっていることなので、転置行列積はこのように表記すると覚えてください）

### トイデータ

作成した関数に、下記の変数を引数として与えてみましょう。（係数は a=2,b=3とします）

```python
X = np.arange(10)
```

戻り値として、下記の出力があれば、正常に作成できています。

```python
y = [3,5,7,9,11,13,15,17,19,21]
```

## まとめ
- 仮定関数はこちら側が仮定するモデル(近似関数)になります
- ここでは近似関数は`y = ax1 + b`と書けるものでした
- 近似関数は転置行列を用いて計算できます