# 線形回帰の学習と推定

## ゴール
- **【Sprint 機械学習スクラッチ 線形回帰】の【問題６】を解くうえで必要な知識や技術について理解する**

### Sprintの目的
- 線形回帰の学習法について理解する
- 学習法の実装法ついて理解する

## どのように学ぶか

【Sprint 機械学習スクラッチ 線形回帰】の【問題6】と照らし合わせながら、進めていきましょう。

## 学習と推定

ここまでの問題では、下記の関数を作成してきました。

`_linear_hypothesis`：仮定関数の出力計算

`_gradient_descent`：$\theta$の更新

この問題では、この2つの関数を利用し(変更可)、冒頭で示した`ScratchLinearRegression`を実装していくことを目的としています。ですので、まだ実装を行っていない`__init__()`と`fit()`を実装してみましょう。

最急降下法の流れに則ると、下記の流れで実装していくとよいでしょう。
1. 学習率や学習回数の初期化＆$\theta$の初期化を`__init__()`で行う（`no_bias`や`verbose`については、アドバンス課題のため考慮しなくても問題ないです）
2. 推定値算出を`_linear_hypothesis`で行う
3. `_gradient_descent`で$\theta$を更新する
ここで毎回損失値を保存しておくことで学習曲線を描けるので、忘れず損失値を計算し、リストで保存しておきましょう。
4. 2,3を学習回数分繰り返す



## まとめ
- これまで作成した関数を利用して学習や値の保持を行います
- 1~4を行うことで、最適な$theta$に更新することができます