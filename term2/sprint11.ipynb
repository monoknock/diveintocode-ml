{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#\n",
    "# 活性化関数 activator\n",
    "#\n",
    "\n",
    "# 全結合層 (Fully Connected Layer, Affine層, Dense層)\n",
    "class FC:\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2) # 行列 shape(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)           # ベクトル shape(1, n_nodes2)\n",
    "        self.Z = None\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = X  # backwardで使う\n",
    "        return X @ self.W + self.B\n",
    "\n",
    "    def backward(self, d_a):\n",
    "        self.dB = np.sum(d_a, axis=0) # なぜsumになるんだ\n",
    "        self.dW = self.Z.T @ d_a      # おそらく微分値がこうなるんだと思う\n",
    "        self.dZ = d_a @ self.W.T      # おそらく微分値がこうなるんだと思う\n",
    "        self.optimizer.update(self)   # 自身のW/Bをoptimizerにより更新\n",
    "        return self.dZ\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        A = self.A\n",
    "        c = 1 / (1 + np.exp(-A))\n",
    "        d = (1 / (1 + np.exp(-A)))**2\n",
    "        return dZ * (c - d)\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - np.tanh(self.A)**2)\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(0, A) # 0とxを比較して大きい方の数値を返す\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.where(self.A > 0, 1, 0)\n",
    "\n",
    "# 更新アルゴリズム optimizer\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.hW += layer.dW * layer.dW # layer側に代入している理由: AdaGradはほかのFCオブジェクトで使い回されるため、FC側に保持する必要がある\n",
    "        layer.hB += layer.dB * layer.dB\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.hW) + 1e-7)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.hB) + 1e-7) # Note. / len(layer.Z)\n",
    "\n",
    "# 初期値 initializer\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2, b_size):\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2, b_size)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return self.sigma * np.random.randn(1, n_nodes2)\n",
    "\n",
    "class XavierInitializer:\n",
    "    # sigmaはinterfaceを合わせるためだけに実装\n",
    "    def __init__(self, sigma):\n",
    "        self.s = None\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.s = np.sqrt(1 / n_nodes1)\n",
    "        return self.s * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return self.s * np.random.randn(1, n_nodes2)\n",
    "\n",
    "class HeInitializer:\n",
    "    # sigmaはinterfaceを合わせるためだけに実装\n",
    "    def __init__(self, sigma):\n",
    "        self.s = None\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        # display(\"n_nodes1: \", n_nodes1)\n",
    "        # display(\"n_nodes2: \", n_nodes2)\n",
    "\n",
    "        self.s = np.sqrt(2 / n_nodes1)\n",
    "        # display(\"self.s: \", self.s)\n",
    "        w = self.s * np.random.randn(n_nodes1, n_nodes2)\n",
    "        # display(\"w: \", w)\n",
    "        return w\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return self.s * np.random.randn(1, n_nodes2)\n",
    "\n",
    "# ミニバッチ\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "# 活性化関数 activator\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W              # 重み\n",
    "        self.b = b              # バイアス\n",
    "        self.stride = stride    # ストライド\n",
    "        self.pad = pad          # ?\n",
    "\n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "\n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "# 活性化関数 activator\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 活性化関数 activator\n",
    "# パディングは考えず、ストライドも1固定\n",
    "# 重みが複数の特徴量に対して共有されている\n",
    "class SimpleConv1d:\n",
    "    def forward(self, x, w, b):\n",
    "        # padding = None\n",
    "        stride = 1\n",
    "        a = []\n",
    "        for i in range(len(w) - 1):\n",
    "            tmp_x = x[i:i+len(w)] # これスカラじゃないの？\n",
    "            a.append(tmp_x @ w + b[0])\n",
    "        return np.array(a)\n",
    "\n",
    "    # 共有されている誤差をすべて足すことで勾配を求める?\n",
    "    def backward(self, x, w, da):\n",
    "        db = np.sum(da)\n",
    "        dw = []\n",
    "        for i in range(len(w)):\n",
    "            dw.append(da @ x[i:i+len(da)])\n",
    "        dw = np.array(dw)\n",
    "        dx = []\n",
    "        new_w = np.insert(w[::-1], 0, 0)\n",
    "        new_w = np.append(new_w, 0)\n",
    "        for i in range(len(new_w)-1):\n",
    "            dx.append(new_w[i:i+len(da)] @ da)\n",
    "        dx = np.array(dx[::-1])\n",
    "        return db, dw, dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題2】1次元畳み込み後の出力サイズの計算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# n_out: 出力の特徴量の数\n",
    "# n_in:  入力の特徴量の数\n",
    "# p:     ある方向へのパディングの数\n",
    "# f:     フィルタのサイズ\n",
    "# s:     すとらいどのサイズ\n",
    "def output_size_calculation(n_in, f, p=0, s=1):\n",
    "    return int((n_in + 2*p - f) / s + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題3】小さな配列での1次元畳み込み層の実験\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 50]\n",
      "30\n",
      "[ 50  80 110]\n",
      "[ 30 110 170 140]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "d_a = np.array([10, 20])\n",
    "\n",
    "conv = SimpleConv1d()\n",
    "print(conv.forward(x, w, b))\n",
    "db, dw, dx = conv.backward(x, w, d_a)\n",
    "print(db)\n",
    "print(dw)\n",
    "print(dx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n"
     ]
    }
   ],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0):\n",
    "        self.b_size = b_size\n",
    "        self.optimizer = optimizer\n",
    "        self.pa = pa\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
    "        self.B = initializer.B(n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.n_in = X.shape[-1]\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa)\n",
    "        X = X.reshape(self.n_in_channels, self.n_in)\n",
    "        self.X = np.pad(X, ((0,0), ((self.b_size-1), 0)))\n",
    "        self.X1 = np.zeros((self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
    "        for i in range(self.b_size):\n",
    "            self.X1[:, i] = np.roll(self.X, -i, axis=-1)\n",
    "        A = np.sum(self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]*self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa, np.newaxis]), axis=-1)\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\n",
    "        self.dA1 = np.zeros((self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
    "        for i in range(self.b_size):\n",
    "            self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W@self.dA1, axis=0)\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "conv = Conv1d(b_size=3, initializer=SimpleInitializer(0.01), optimizer=SGD(0.01), n_in_channels=2, n_out_channels=3, pa=0)\n",
    "conv.W = np.ones((3, 2, 3), dtype=float)\n",
    "conv.B = np.array([1, 2, 3], dtype=float)\n",
    "result = conv.forward(x)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題8】学習と推定"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, Y):\n",
    "        self.loss = self.loss_func(Y)\n",
    "        return self.Z - Y\n",
    "\n",
    "    def loss_func(self, Y, Z=None):\n",
    "        if Z is None:\n",
    "            Z = self.Z\n",
    "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "\n",
    "class Conv1d2:\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n",
    "        self.b_size = b_size\n",
    "        self.optimizer = optimizer\n",
    "        self.pa = pa\n",
    "        self.stride = stride\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
    "        self.B = initializer.B(n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_in = X.shape[-1]\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n",
    "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
    "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
    "        for i in range(self.b_size):\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
    "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
    "        for i in range(self.b_size):\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "# スクラッチクラス\n",
    "class ScratchCNNClassifier:\n",
    "\n",
    "    def __init__(self, epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, Activater=Tanh, Optimizer=AdaGrad):\n",
    "        self.sigma = 0.02\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.Activater = Activater\n",
    "        if Activater == Sigmoid or Activater == Tanh:\n",
    "            self.Initializer = XavierInitializer\n",
    "        elif Activater == ReLU:\n",
    "            self.Initializer = HeInitializer\n",
    "        self.Optimizer = Optimizer\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.val_enable = False\n",
    "        if X_val is not None:\n",
    "            self.val_enable = True\n",
    "        self.Conv1d = Conv1d2(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
    "        self.Conv1d.n_out = output_size_calculation(X.shape[-1], self.Conv1d.b_size, self.Conv1d.pa, self.Conv1d.stride)\n",
    "        self.activation1 = self.Activater()\n",
    "        self.FC2 = FC(1*self.Conv1d.n_out, self.n_nodes2, self.Initializer(self.sigma), self.Optimizer(self.lr))\n",
    "        self.activation2 = self.Activater()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(self.sigma), self.Optimizer(self.lr))\n",
    "        self.activation3 = Softmax()\n",
    "\n",
    "        self.loss = []\n",
    "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            print(f\"epoch: {i}\")\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            self.iter = len(get_mini_batch)\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                self.forward_propagation(mini_X)\n",
    "                self.back_propagation(mini_X, mini_y)\n",
    "                self.loss.append(self.activation3.loss)\n",
    "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward_propagation(X), axis=1)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        A1 = self.Conv1d.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "\n",
    "    def back_propagation(self, X, y_true):\n",
    "        dA3 = self.activation3.backward(y_true)\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dA1 = dA1[:, np.newaxis]\n",
    "        dZ0 = self.Conv1d.backward(dA1)\n",
    "\n",
    "#\n",
    "# 前処理\n",
    "#\n",
    "\n",
    "# データセット\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # shape(60000, 28, 28)\n",
    "\n",
    "# 平滑化\n",
    "x_train = x_train.reshape(-1, 784) # shape(60000, 784)\n",
    "x_test = x_test.reshape(-1, 784)   # shape(60000, 784)\n",
    "# スケール調整\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "# one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "def result(model, x_train, y_train, x_test, y_test):\n",
    "    # 学習結果\n",
    "    # display(model.loss)\n",
    "\n",
    "    # 予測\n",
    "    pred_train = model.predict(x_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "\n",
    "    # 評価\n",
    "    print(\"train accuracy_score: \\t\", accuracy_score(y_train, pred_train))\n",
    "    print(\"train precision_score: \\t\", precision_score(y_train, pred_train, average=\"micro\"))\n",
    "    print(\"train recall_score: \\t\", recall_score(y_train, pred_train, average=\"micro\"))\n",
    "    print(\"train f1_score: \\t\", f1_score(y_train, pred_train, average=\"micro\"))\n",
    "    print(\"train accuracy_score: \\t\", accuracy_score(y_test, pred_test))\n",
    "    print(\"train precision_score: \\t\", precision_score(y_test, pred_test, average=\"micro\"))\n",
    "    print(\"train recall_score: \\t\", recall_score(y_test, pred_test, average=\"micro\"))\n",
    "    print(\"train f1_score: \\t\", f1_score(y_test, pred_test, average=\"micro\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "elapsed_time: 72 [sec]\n"
     ]
    }
   ],
   "source": [
    "# 学習 AdaGrad, Xavier, Tanh\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "m = ScratchCNNClassifier(epoch=5, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, Activater=Tanh, Optimizer=SGD)\n",
    "m.fit(x_train, y_train_one_hot, x_test, y_test_one_hot)\n",
    "\n",
    "print (\"elapsed_time: {} \".format(int(time.time() - start)) + \"[sec]\")\n",
    "\n",
    "# y_pred = test3.predict(x_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy_score: \t 0.9870833333333333\n",
      "train precision_score: \t 0.9870833333333333\n",
      "train recall_score: \t 0.9870833333333333\n",
      "train f1_score: \t 0.9870833333333333\n",
      "train accuracy_score: \t 0.9765\n",
      "train precision_score: \t 0.9765\n",
      "train recall_score: \t 0.9765\n",
      "train f1_score: \t 0.9765\n"
     ]
    }
   ],
   "source": [
    "result(m, x_train, y_train, x_test, y_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}