{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 10 - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "        self.optimizer = optimizer\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.Z = X\n",
    "        self.A = X @ self.W + self.B\n",
    "#         display(\"X\")\n",
    "#         display(X[0][:300])\n",
    "#         display(\"self.W\")\n",
    "#         display(self.W)\n",
    "#         display(\"self.B\")\n",
    "#         display(self.B)\n",
    "\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.Z.T @ dA\n",
    "        self.dZ = dA @ self.W.T\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW / len(layer.Z)\n",
    "        layer.B -= self.lr * layer.dB / len(layer.Z)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = 1 / (1 + np.exp(-self.A))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * ((1 / (1 + np.exp(-self.A))) - (1 / (1 + np.exp(-self.A)))**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - np.tanh(self.A)**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "\n",
    "    def forward(self, A): \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        return Z\n",
    "        \n",
    "    def backward(self, Z, y):\n",
    "        dA = Z - y\n",
    "        loss = - np.sum(y * np.log(Z)) / len(y)\n",
    "        return dA, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        _ = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "        \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "\n",
    "    def __init__(self, sigma):\n",
    "        _ = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "#         display(\"n_nodes1: \", n_nodes1)\n",
    "#         display(\"n_nodes2: \", n_nodes2)\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "#         display(\"self.s: \", self.sigma)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "#         display(\"w: \", W)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr \n",
    "    \n",
    "    def update(self, layer):\n",
    "        display(layer.dW.shape)\n",
    "        layer.HW += layer.dW * layer.dW\n",
    "        layer.HB += layer.dB * layer.dB\n",
    "        delta = 1e-7\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + delta) / len(layer.Z)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.HB) + delta) / len(layer.Z)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "\n",
    "    def __init__(self, verbose=False, epoch=1, optimizer=SGD, initializer=HeInitializer, activater=ReLU):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = 20 \n",
    "        self.n_features = 784 \n",
    "        self.n_nodes1 = 400 \n",
    "        self.n_nodes2 = 200 \n",
    "        self.n_output = 10 \n",
    "        self.sigma = 0.02 \n",
    "        self.lr = 0.5 \n",
    "        self.epoch = epoch \n",
    "        self.optimizer = optimizer \n",
    "        self.initializer = initializer \n",
    "        self.activater = activater \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.loss_train = [] \n",
    "        self.loss_val = [] \n",
    "        optimizer = self.optimizer(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
    "        self.activation1 = self.activater()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
    "        self.activation2 = self.activater()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer(self.sigma), optimizer)\n",
    "        self.activation3 = softmax()\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "#                 print(\"mini_X\")\n",
    "#                 print(mini_X[0][:300])\n",
    "                A1 = self.FC1.forward(mini_X)\n",
    "#                 print(\"A1\")\n",
    "#                 print(A1)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "#                 print(\"Z1\")\n",
    "#                 print(Z1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "#                 print(\"A2\")\n",
    "#                 print(A2)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "#                 print(\"Z2\")\n",
    "#                 print(Z2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "#                 print(\"Z2\")\n",
    "#                 print(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                dA3, loss = self.activation3.backward(Z3, mini_y) \n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) \n",
    "\n",
    "            if self.verbose:\n",
    "                A1 = self.FC1.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)            \n",
    "                self.loss_train.append(self.activation3.backward(Z3, y)[1])\n",
    "                \n",
    "                if X_val is not None:\n",
    "                    A1 = self.FC1.forward(X_val)\n",
    "                    Z1 = self.activation1.forward(A1)\n",
    "                    A2 = self.FC2.forward(Z1)\n",
    "                    Z2 = self.activation2.forward(A2)\n",
    "                    A3 = self.FC3.forward(Z2)\n",
    "                    Z3 = self.activation3.forward(A3)            \n",
    "                    self.loss_val.append(self.activation3.backward(Z3, y_val)[1])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_val[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(X_train[:1].shape)\n",
    "# display(X_train[:1][0][300:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(400, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(784, 400)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SDNN = ScratchDeepNeuralNetrowkClassifier(verbose=True, epoch=1, optimizer=AdaGrad, initializer=HeInitializer, activater=ReLU) \n",
    "\n",
    "SDNN.fit(X_train[:20], y_train_one_hot[:20], X_val, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = SDNN.predict(X_val)\n",
    "# accuracy_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(list(range(1, SDNN.epoch+1)), SDNN.loss_train, label='train')\n",
    "# plt.plot(list(range(1, SDNN.epoch+1)), SDNN.loss_val, label='test')\n",
    "# plt.legend()\n",
    "# plt.xticks(list(range(1, SDNN.epoch+1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check the accuracy with other layers. We have tested with three layers, let's check for 4 and 6 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScratchDeepNeuralNetrowkClassifier_4():\n",
    "\n",
    "#     def __init__(self, verbose=False, epoch=1, optimizer=SGD, initializer=HeInitializer, activater=ReLU):\n",
    "#         self.verbose = verbose\n",
    "#         self.batch_size = 20 \n",
    "#         self.n_features = 784 \n",
    "#         self.n_nodes1 = 400 \n",
    "#         self.n_nodes2 = 200 \n",
    "#         self.n_nodes3 = 150 \n",
    "#         self.n_output = 10 \n",
    "#         self.sigma = 0.02 \n",
    "#         self.lr = 0.5 \n",
    "#         self.epoch = epoch \n",
    "#         self.optimizer = optimizer \n",
    "#         self.initializer = initializer\n",
    "#         self.activater = activater \n",
    "    \n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         self.loss_train = [] \n",
    "#         self.loss_val = []\n",
    "#         optimizer = self.optimizer(self.lr)\n",
    "\n",
    "#         self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation1 = self.activater()\n",
    "#         self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation2 = self.activater()\n",
    "#         self.FC3 = FC(self.n_nodes2, self.n_nodes3, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation3 = self.activater()\n",
    "#         self.FC4 = FC(self.n_nodes3, self.n_output, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation4 = softmax()\n",
    "        \n",
    "#         for i in range(self.epoch):\n",
    "#             get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
    "#             for mini_X, mini_y in get_mini_batch:\n",
    "#                 self.forward(mini_X)\n",
    "#                 self.backward(mini_y)\n",
    "            \n",
    "#             if self.verbose:\n",
    "#                 self.forward(X)\n",
    "#                 self.loss_train.append(self.activation4.backward(self.Z4, y)[1])\n",
    "                \n",
    "#                 if X_val is not None:\n",
    "#                     self.forward(X_val)\n",
    "#                     self.loss_val.append(self.activation4.backward(self.Z4, y_val)[1])\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         A1 = self.FC1.forward(X)\n",
    "#         Z1 = self.activation1.forward(A1)\n",
    "#         A2 = self.FC2.forward(Z1)\n",
    "#         Z2 = self.activation2.forward(A2)\n",
    "#         A3 = self.FC3.forward(Z2)\n",
    "#         Z3 = self.activation3.forward(A3)\n",
    "#         A4 = self.FC4.forward(Z3)\n",
    "#         self.Z4 = self.activation4.forward(A4)\n",
    "        \n",
    "#     def backward(self, y):\n",
    "#         dA4, self.loss = self.activation4.backward(self.Z4, y) \n",
    "#         dZ3 = self.FC4.backward(dA4)\n",
    "#         dA3 = self.activation3.backward(dZ3)\n",
    "#         dZ2 = self.FC3.backward(dA3)\n",
    "#         dA2 = self.activation2.backward(dZ2)\n",
    "#         dZ1 = self.FC2.backward(dA2)\n",
    "#         dA1 = self.activation1.backward(dZ1)\n",
    "#         dZ0 = self.FC1.backward(dA1) \n",
    "        \n",
    "#     def predict(self, X):\n",
    "#         self.forward(X)\n",
    "#         return np.argmax(self.Z4, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDNN4 = ScratchDeepNeuralNetrowkClassifier_4(verbose=True, epoch=10, optimizer=AdaGrad, initializer=HeInitializer, activater=ReLU)\n",
    "# SDNN4.fit(X_train, y_train_one_hot, X_val, y_test_one_hot)\n",
    "\n",
    "# pred = SDNN4.predict(X_val)\n",
    "# accuracy_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(1, SDNN4.epoch+1)), SDNN4.loss_train, label='train')\n",
    "# plt.plot(list(range(1, SDNN4.epoch+1)), SDNN4.loss_val, label='test')\n",
    "# plt.legend()\n",
    "# plt.xticks(list(range(1, SDNN4.epoch+1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScratchDeepNeuralNetrowkClassifier_6():\n",
    "\n",
    "#     def __init__(self, verbose=False, epoch=1, optimizer=SGD, initializer=HeInitializer, activater=ReLU):\n",
    "#         self.verbose = verbose\n",
    "#         self.batch_size = 20 \n",
    "#         self.n_features = 784 \n",
    "#         self.n_nodes1 = 400 \n",
    "#         self.n_nodes2 = 200 \n",
    "#         self.n_nodes3 = 150 \n",
    "#         self.n_nodes4 = 100 \n",
    "#         self.n_nodes5 = 50 \n",
    "#         self.n_output = 10 \n",
    "#         self.sigma = 0.02 \n",
    "#         self.lr = 0.5 \n",
    "#         self.epoch = epoch \n",
    "#         self.optimizer = optimizer \n",
    "#         self.initializer = initializer \n",
    "#         self.activater = activater \n",
    "    \n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         self.loss_train = [] \n",
    "#         self.loss_val = []\n",
    "#         optimizer = self.optimizer(self.lr)\n",
    "\n",
    "#         self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation1 = self.activater()\n",
    "#         self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation2 = self.activater()\n",
    "#         self.FC3 = FC(self.n_nodes2, self.n_nodes3, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation3 = self.activater()\n",
    "#         self.FC4 = FC(self.n_nodes3, self.n_nodes4, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation4 = self.activater()\n",
    "#         self.FC5 = FC(self.n_nodes4, self.n_nodes5, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation5 = self.activater()\n",
    "#         self.FC6 = FC(self.n_nodes5, self.n_output, self.initializer(self.sigma), optimizer)\n",
    "#         self.activation6 = softmax()\n",
    "        \n",
    "#         for i in range(self.epoch):\n",
    "#             get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
    "#             for mini_X, mini_y in get_mini_batch:\n",
    "#                 self.forward(mini_X)\n",
    "#                 self.backward(mini_y)\n",
    "            \n",
    "#             if self.verbose:\n",
    "#                 self.forward(X)\n",
    "#                 self.loss_train.append(self.activation6.backward(self.Z6, y)[1])\n",
    "                \n",
    "#                 if X_val is not None:\n",
    "#                     self.forward(X_val)\n",
    "#                     self.loss_val.append(self.activation6.backward(self.Z6, y_val)[1])\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         A1 = self.FC1.forward(X)\n",
    "#         Z1 = self.activation1.forward(A1)\n",
    "#         A2 = self.FC2.forward(Z1)\n",
    "#         Z2 = self.activation2.forward(A2)\n",
    "#         A3 = self.FC3.forward(Z2)\n",
    "#         Z3 = self.activation3.forward(A3)\n",
    "#         A4 = self.FC4.forward(Z3)\n",
    "#         Z4 = self.activation4.forward(A4)\n",
    "#         A5 = self.FC5.forward(Z4)\n",
    "#         Z5 = self.activation5.forward(A5)\n",
    "#         A6 = self.FC6.forward(Z5)\n",
    "#         self.Z6 = self.activation6.forward(A6)\n",
    "        \n",
    "#     def backward(self, y):\n",
    "#         dA6, self.loss = self.activation6.backward(self.Z6, y) \n",
    "#         dZ5 = self.FC6.backward(dA6)\n",
    "#         dA5 = self.activation5.backward(dZ5)\n",
    "#         dZ4 = self.FC5.backward(dA5)\n",
    "#         dA4 = self.activation4.backward(dZ4)\n",
    "#         dZ3 = self.FC4.backward(dA4)\n",
    "#         dA3 = self.activation3.backward(dZ3)\n",
    "#         dZ2 = self.FC3.backward(dA3)\n",
    "#         dA2 = self.activation2.backward(dZ2)\n",
    "#         dZ1 = self.FC2.backward(dA2)\n",
    "#         dA1 = self.activation1.backward(dZ1)\n",
    "#         dZ0 = self.FC1.backward(dA1) \n",
    "        \n",
    "#     def predict(self, X):\n",
    "#         self.forward(X)\n",
    "#         return np.argmax(self.Z6, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDNN6 = ScratchDeepNeuralNetrowkClassifier_6(verbose=True, epoch=10, optimizer=AdaGrad, initializer=HeInitializer, activater=ReLU)\n",
    "# SDNN6.fit(X_train, y_train_one_hot, X_val, y_test_one_hot)\n",
    "\n",
    "# pred = SDNN6.predict(X_val)\n",
    "# accuracy_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(list(range(1, SDNN6.epoch+1)), SDNN6.loss_train, label='train')\n",
    "# plt.plot(list(range(1, SDNN6.epoch+1)), SDNN6.loss_val, label='test')\n",
    "# plt.legend()\n",
    "# plt.xticks(list(range(1, SDNN6.epoch+1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In conclusion, we can say that the accuracy was at its best when we had three layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally generalize the ScratchDeepNeuralNetrowkClassifier and make it possible to input layers with the number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class General_ScratchDeepNeuralNetrowkClassifier():\n",
    "\n",
    "#     def __init__(self, verbose=False, epoch=1, optimizer=SGD, initializer=HeInitializer, activater=ReLU, n_nodes=None):\n",
    "#         self.verbose = verbose\n",
    "#         self.batch_size = 20 \n",
    "#         self.sigma = 0.02\n",
    "#         self.lr = 0.5 \n",
    "#         self.epoch = epoch \n",
    "#         self.optimizer = optimizer \n",
    "#         self.initializer = initializer \n",
    "#         self.activater = activater \n",
    "#         self.n_nodes = n_nodes \n",
    "    \n",
    "#     def fit(self, X, y, X_val=None, y_val=None):\n",
    "#         self.loss_train = [] \n",
    "#         self.loss_val = [] \n",
    "#         optimizer = self.optimizer(self.lr)\n",
    "#         self.fcs = [] \n",
    "#         self.act = [] \n",
    "        \n",
    "#         for i in range(len(self.n_nodes)-2):\n",
    "#             self.fcs.append(FC(self.n_nodes[i], self.n_nodes[i+1], self.initializer(self.sigma), optimizer))\n",
    "#             self.act.append(self.activater())\n",
    "#         self.fcs.append(FC(self.n_nodes[i+1], self.n_nodes[-1], self.initializer(self.sigma), optimizer))\n",
    "#         self.act.append(softmax())\n",
    "\n",
    "#         for i in range(self.epoch):\n",
    "#             get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
    "#             for mini_X, mini_y in get_mini_batch:\n",
    "#                 A = []\n",
    "#                 Z = []\n",
    "#                 for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
    "#                     if i == 0:\n",
    "#                         A.append(f.forward(mini_X))\n",
    "#                         Z.append(a.forward(A[i]))\n",
    "#                     else:\n",
    "#                         A.append(f.forward(Z[i-1]))\n",
    "#                         Z.append(a.forward(A[i]))     \n",
    "#                 dA = []\n",
    "#                 dZ = []\n",
    "#                 for i, (f, a) in enumerate(zip(self.fcs[::-1], self.act[::-1])):\n",
    "#                     if i == 0:\n",
    "#                         dA.append(a.backward(Z[-(i+1)], mini_y)[0])\n",
    "#                         dZ.append(f.backward(dA[i]))\n",
    "#                     else:\n",
    "#                         dA.append(a.backward(dZ[i-1]))\n",
    "#                         dZ.append(f.backward(dA[i]))\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 A = []\n",
    "#                 Z = []\n",
    "#                 for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
    "#                     if i == 0:\n",
    "#                         A.append(f.forward(X))\n",
    "#                         Z.append(a.forward(A[i]))\n",
    "#                     else:\n",
    "#                         A.append(f.forward(Z[i-1]))\n",
    "#                         Z.append(a.forward(A[i]))           \n",
    "#                 self.loss_train.append(self.act[-1].backward(Z[-1], y)[1])\n",
    "                \n",
    "#                 if X_val is not None:\n",
    "#                     A = []\n",
    "#                     Z = []\n",
    "#                     for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
    "#                         if i == 0:\n",
    "#                             A.append(f.forward(X_val))\n",
    "#                             Z.append(a.forward(A[i]))\n",
    "#                         else:\n",
    "#                             A.append(f.forward(Z[i-1]))\n",
    "#                             Z.append(a.forward(A[i]))           \n",
    "#                     self.loss_val.append(self.act[-1].backward(Z[-1], y_val)[1])\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         A = []\n",
    "#         Z = []\n",
    "#         for i, (f, a) in enumerate(zip(self.fcs, self.act)):\n",
    "#             if i == 0:\n",
    "#                 A.append(f.forward(X))\n",
    "#                 Z.append(a.forward(A[i]))\n",
    "#             else:\n",
    "#                 A.append(f.forward(Z[i-1]))\n",
    "#                 Z.append(a.forward(A[i]))\n",
    "#         return np.argmax(Z[-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's test it with 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_list = [784, 400, 200, 150, 100, 10]\n",
    "# SDNN5 = General_ScratchDeepNeuralNetrowkClassifier(verbose=True, epoch=10, optimizer=AdaGrad, initializer=HeInitializer, activater=ReLU, n_nodes=node_list)\n",
    "# SDNN5.fit(X_train, y_train_one_hot, X_val, y_test_one_hot)\n",
    "# pred = SDNN5.predict(X_val)\n",
    "# accuracy_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(1, SDNN5.epoch+1)), SDNN5.loss_train, label='train')\n",
    "# plt.plot(list(range(1, SDNN5.epoch+1)), SDNN5.loss_val, label='test')\n",
    "# plt.legend()\n",
    "# plt.xticks(list(range(1, SDNN5.epoch+1)));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
